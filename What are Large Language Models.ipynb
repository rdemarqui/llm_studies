{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPo61V44Ds+E1FHFGAu6RZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdemarqui/llm_studies/blob/main/What%20are%20Large%20Language%20Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3VV2oqEQceja"
      },
      "outputs": [],
      "source": [
        "KEY=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are Large Language Models?"
      ],
      "metadata": {
        "id": "c2zaAoJEd9yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let’s Generate Some Text"
      ],
      "metadata": {
        "id": "NS6rseZUfaz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q"
      ],
      "metadata": {
        "id": "XaDgen07dDNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = KEY"
      ],
      "metadata": {
        "id": "NjlJxToKddO8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from dotenv import load_dotenv\n",
        "#load_dotenv()\n",
        "import os\n",
        "import openai\n",
        "\n",
        "# English text to translate\n",
        "english_text = \"Hello, how are you?\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": f'Translate the following English text to French: \"{english_text}\"'}\n",
        "  ],\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdgpL0arc3lc",
        "outputId": "4627ca95-6075-4ec7-c611-5661a59169cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Bonjour, comment ça va ?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-Shot Learning"
      ],
      "metadata": {
        "id": "pTEpfC3Gf2uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot learning in the context of LLMs refers to providing the model with a few examples before making predictions. These examples \"teach\" the model how to reason and act as \"filters\" to help the model search for relevant patterns in the dataset.\n",
        "\n",
        "The idea of few-shot learning is fascinating as it suggests that the model can be quickly reprogrammed for new tasks. While LLMs like GPT3 excel at language modeling tasks like machine translation, they may struggle with more complex reasoning tasks.\n",
        "\n",
        "The few-shot examples are helping the model search for relevant patterns in the dataset. The dataset, which is effectively compressed into the model's weights, can be searched for patterns that strongly respond to these provided examples. These patterns are then used to generate the model's output. The more examples provided, the more precise the output becomes.\n",
        "\n",
        "Here’s an example of few-shot learning:"
      ],
      "metadata": {
        "id": "-B9HK2RcgKtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for summarization\n",
        "prompt = \"\"\"\n",
        "Describe the following movie using emojis.\n",
        "\n",
        "{movie}: \"\"\"\n",
        "\n",
        "examples = [\n",
        "\t{ \"input\": \"Titanic\", \"output\": \"🛳️🌊❤️🧊🎶🔥🚢💔👫💑\" },\n",
        "\t{ \"input\": \"The Matrix\", \"output\": \"🕶️💊💥👾🔮🌃👨🏻‍💻🔁🔓💪\" }\n",
        "]\n",
        "\n",
        "movie = \"Toy Story\"\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt.format(movie=examples[0][\"input\"])},\n",
        "        {\"role\": \"assistant\", \"content\": examples[0][\"output\"]},\n",
        "        {\"role\": \"user\", \"content\": prompt.format(movie=examples[1][\"input\"])},\n",
        "        {\"role\": \"assistant\", \"content\": examples[1][\"output\"]},\n",
        "        {\"role\": \"user\", \"content\": prompt.format(movie=movie)},\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ycXay5f3oK",
        "outputId": "8bf5230d-1fa9-4c30-cbe0-90d9ff8b115a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧸🌈🤠👨‍🚀👨‍🎤🚀🎢🧸💔💕🌟\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling Laws\n"
      ],
      "metadata": {
        "id": "gMjTOgvzg0cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling laws refer to the relationship between the model's performance and factors such as the number of parameters, the size of the training dataset, the compute budget, and the network architecture. They were discovered after a lot of experiments and are described in the Chinchilla paper. These laws provide insights into how to allocate resources when training these models optimally.\n",
        "\n",
        "The main elements characterizing a language model are:\n",
        "\n",
        "The number of parameters (N) reflects the model's capacity to learn from data. More parameters allow the model to capture complex patterns in the data.\n",
        "The size of the training dataset (D) is measured in the number of tokens (small pieces of text ranging from a few words to a single character).\n",
        "FLOPs (floating point operations per second) measure the compute budget used for training.\n",
        "The researchers trained the Chinchilla model, which has 70B parameters, on 1.4 trillion tokens. This aligns with the rule of thumb proposed in the paper:  for a model with X parameters, it is optimal to train it on approximately X * 20 tokens. For example, in the context of this rule, a model with 100 billion parameters would be optimally trained on approximately 2 trillion tokens.\n",
        "\n",
        "Applying this rule, the Chinchilla model, though smaller, performed better than other LLMs. It showed gains in language modeling and task performance and needed less memory and computing power. You can read more about Chinchilla in its paper “Training Compute-Optimal Large Language Models”."
      ],
      "metadata": {
        "id": "_buyBfPwg3-m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qp9mRuHQgkIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}